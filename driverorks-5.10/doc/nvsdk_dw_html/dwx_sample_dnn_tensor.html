<!-- HTML header for doxygen 1.8.7-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<!--
 * Copyright (c) 2009-2014 NVIDIA CORPORATION.  All rights reserved.
 *
 * NVIDIA Corporation and its licensors retain all intellectual property
 * and proprietary rights in and to this software, related documentation
 * and any modifications thereto.  Any use, reproduction, disclosure or
 * distribution of this software and related documentation without an express
 * license agreement from NVIDIA Corporation is strictly prohibited.
-->
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.4"/>
<title>DriveWorks SDK Reference: Basic Object Detector Using DNN Tensor Sample</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="shortcut icon" href="Nvidia.ico" type="image/x-icon" />
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="reverb-search.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
<link href="nv.css" rel="stylesheet" type="text/css" />
<link href="nvdwx.css" rel="stylesheet" type="text/css"/>
<style>
 body {
 background-position: 350px 150px;
 background-image: url(watermark.png);
 background-repeat: no-repeat;
 background-attachment: fixed;
 }
 </style>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table width="99%" border="0" cellspacing="1" cellpadding="1">
  <tbody>
    <tr valign="middle">
      <td rowspan="2" height="44" width="19%">
        <div>
            <a id="nv-logo" href="https://www.nvidia.com/"></a>
        </div>
      <td width="81%" height="44">
        <div style="text-align:right; font-weight: bold; font-size:20px"> <br/>DriveWorks SDK Reference </div>
        <div style="text-align:right">
        5.10.87 Release <br/> For Test and Development only <br/> <br/> </div>
    </td>
  </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.4 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('dwx_sample_dnn_tensor.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Basic Object Detector Using DNN Tensor Sample </div></div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#dwx_sample_dnn_tensor_description">Description</a></li>
<li class="level1"><a href="#dwx_sample_dnn_tensor_sample_running">Running the Sample</a><ul><li class="level2"><a href="#dwx_sample_dnn_tensor_sample_examples">Examples</a></li>
</ul>
</li>
<li class="level1"><a href="#dwx_sample_dnn_tensor_sample_output">Output</a></li>
<li class="level1"><a href="#dwx_sample_dnn_tensor_sample_more">Additional Information</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="dwx_sample_dnn_tensor_description"></a>
Description</h1>
<p >The Basic Object Detector Using DNN Tensor sample demonstrates how the <a class="el" href="group__dnn__group.html">DNN Interface</a> can be used for object detection using <code><a class="el" href="group__dnntensor__group.html#structdwDNNTensor" title="Exposes the content of a dwDNNTensorHandle_t.">dwDNNTensor</a></code>.</p>
<p >The sample streams a H.264 or RAW video and runs DNN inference on each frame to detect objects using NVIDIA<sup>&reg;</sup> TensorRT<sup>&trade;</sup> model.</p>
<p >The interpretation of the output of a network depends on the network design. In this sample, 2 output blobs (with <code>coverage</code> and <code>bboxes</code> as blob names) are interpreted as coverage and bounding boxes.</p>
<p >For each frame, it detects the object locations, uses <code>dwDNNTensorStreamer</code> to get the output of the inference from GPU to CPU, and finally traverses through the tensor to extract the detections.</p>
<h1><a class="anchor" id="dwx_sample_dnn_tensor_sample_running"></a>
Running the Sample</h1>
<p >The Basic Object Detector Using <a class="el" href="group__dnntensor__group.html#structdwDNNTensor" title="Exposes the content of a dwDNNTensorHandle_t.">dwDNNTensor</a> sample, <code>sample_dnn_tensor</code>, accepts the following optional parameters. If none are specified, it performs detections on a supplied pre-recorded video. </p><pre class="fragment">./sample_dnn_tensor --input-type=[video|camera]
                    --video=[path/to/video]
                    --camera-type=[camera]
                    --camera-group=[a|b|c|d]
                    --camera-index=[0|1|2|3]
                    --tensorRT_model=[path/to/TensorRT/model]
</pre><p> Where: </p><pre class="fragment">--input-type=[video|camera]
        Defines if the input is from live camera or from a recorded video.
        Live camera is supported only on NVIDIA DRIVE platforms.
        Default value: video

--video=[path/to/video]
        Specifies the absolute or relative path of a raw or h264 recording.
        Only applicable if --input-type=video
        Default value: path/to/data/samples/sfm/triangulation/video_0.h264.

--camera-type=[camera]
        Specifies a supported AR0231 `RCCB` sensor.
        Only applicable if --input-type=camera.
        Default value: ar0231-rccb-bae-sf3324

--camera-group=[a|b|c|d]
        Specifies the group where the camera is connected to.
        Only applicable if --input-type=camera.
        Default value: a

--camera-index=[0|1|2|3]
        Specifies the camera index on the given port.
        Default value: 0

--tensorRT_model=[path/to/TensorRT/model]
        Specifies the path to the NVIDIA&lt;sup&gt;&amp;reg;&lt;/sup&gt; TensorRT&lt;sup&gt;&amp;trade;&lt;/sup&gt;
        model file.
        The loaded network is expected to have a coverage output blob named "coverage" and a bounding box output blob named "bboxes".
        Default value: path/to/data/samples/detector/&lt;gpu-architecture&gt;/tensorRT_model.bin, where &lt;gpu-architecture&gt; can be `pascal` or `volta-discrete` or `volta-integrated` or `turing`.
</pre><dl class="section note"><dt>Note</dt><dd>This sample loads its DataConditioner parameters from DNN metadata JSON file. To provide the DNN metadata to the DNN module, place the JSON file in the same directory as the model file. An example of the DNN metadata file is: <pre class="fragment">data/samples/detector/pascal/tensorRT_model.bin.json
</pre> </dd></dl>
<h2><a class="anchor" id="dwx_sample_dnn_tensor_sample_examples"></a>
Examples</h2>
<h4>Default usage</h4>
<pre class="fragment"> ./sample_dnn_tensor
</pre><p> The video file must be a H.264 or RAW stream. Video containers such as MP4, AVI, MKV, etc. are not supported.</p>
<h4>To run the sample on a video on NVIDIA DRIVE or Linux platforms with a custom TensorRT network</h4>
<pre class="fragment">./sample_dnn_tensor --input-type=video --video=&lt;video file.h264/raw&gt; --tensorRT_model=&lt;TensorRT model file&gt;
</pre> <h4>To run the sample on a camera on NVIDIA DRIVE platforms with a custom TensorRT network</h4>
<pre class="fragment">./sample_dnn_tensor --input-type=camera --camera-type=&lt;rccb_camera_type&gt; --camera-group=&lt;camera group&gt; --camera-index=&lt;camera idx on camera group&gt; --tensorRT_model=&lt;TensorRT model file&gt;
</pre><p> where <code>&lt;rccb_camera_type&gt;</code> is a supported <code>RCCB</code> sensor. See <a class="el" href="supported_sensors.html">Cameras Supported</a> for the list of supported cameras for each platform.</p>
<h1><a class="anchor" id="dwx_sample_dnn_tensor_sample_output"></a>
Output</h1>
<p >The sample creates a window, displays the video streams, and overlays detected bounding boxes of the objects.</p>
<p >The color coding of the overlay is:</p>
<ul>
<li>Red bounding boxes: Indicate detected objects.</li>
<li>Yellow bounding box: Identifies the region which is given as input to the DNN.</li>
</ul>
<div class="image">
<img src="sample_dnn_tensor.png" alt=""/>
<div class="caption">
Object detector using DNN Tensor on a H.264 stream</div></div>
    <h1><a class="anchor" id="dwx_sample_dnn_tensor_sample_more"></a>
Additional Information</h1>
<p >For more information, see:</p><ul>
<li><a class="el" href="dnn_mainsection.html">DNN</a></li>
<li><a class="el" href="dataconditioner_mainsection.html">Data Conditioner</a> </li>
</ul>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->

  <div id="nav-path" class="navpath">
    <ul>
      <li class="footer">
        Advance Information | Subject to Change |
        Prepared and Provided under NDA | Generated by NVIDIA |
        PR-08397-V5.0
      </li>
     </ul>
  </div>
</body>
</html>
