<!-- HTML header for doxygen 1.8.7-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<!--
 * Copyright (c) 2009-2014 NVIDIA CORPORATION.  All rights reserved.
 *
 * NVIDIA Corporation and its licensors retain all intellectual property
 * and proprietary rights in and to this software, related documentation
 * and any modifications thereto.  Any use, reproduction, disclosure or
 * distribution of this software and related documentation without an express
 * license agreement from NVIDIA Corporation is strictly prohibited.
-->
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.4"/>
<title>DriveWorks SDK Reference: Camera</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="shortcut icon" href="Nvidia.ico" type="image/x-icon" />
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="reverb-search.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
<link href="nv.css" rel="stylesheet" type="text/css" />
<link href="nvdwx.css" rel="stylesheet" type="text/css"/>
<style>
 body {
 background-position: 350px 150px;
 background-image: url(watermark.png);
 background-repeat: no-repeat;
 background-attachment: fixed;
 }
 </style>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table width="99%" border="0" cellspacing="1" cellpadding="1">
  <tbody>
    <tr valign="middle">
      <td rowspan="2" height="44" width="19%">
        <div>
            <a id="nv-logo" href="https://www.nvidia.com/"></a>
        </div>
      <td width="81%" height="44">
        <div style="text-align:right; font-weight: bold; font-size:20px"> <br/>DriveWorks SDK Reference </div>
        <div style="text-align:right">
        5.10.87 Release <br/> For Test and Development only <br/> <br/> </div>
    </td>
  </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.4 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('camera_mainsection.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Camera </div></div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#camera_mainsection_about">About This Module</a></li>
<li class="level1"><a href="#camera_mainsection_camera_creation">Creating and Using Camera</a><ul><li class="level2"><a href="#camera_mainsection_gmsl_cameras">GMSL Cameras (camera.gmsl)</a><ul><li class="level3"><a href="#gmsl_camera_logging">Relevant logging</a></li>
</ul>
</li>
<li class="level2"><a href="#camera_mainsection_virtual">Virtual Cameras (camera.virtual)</a></li>
<li class="level2"><a href="#camera_mainsection_gmsl_usb_ptgrey">USB Cameras (camera.usb)</a></li>
</ul>
</li>
<li class="level1"><a href="#camera_mainsection_gmsl_use_cases">Relevant Tutorials</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="camera_mainsection_about"></a>
About This Module</h1>
<p >The Camera module defines a common interface that expands the categories of cameras that NVIDIA<sup>&reg;</sup> DriveWorks supports.</p>
<p >The Camera module supports both GMSL and USB interfaces. It also provides the ability to replay recorded camera data through the DriveWorks stack as a virtual sensor.</p>
<p >A selection of GMSL cameras are supported out-of-the-box; please refer to <a href="https://developer.nvidia.com/drive/ecosystem-hw-sw">https://developer.nvidia.com/drive/ecosystem-hw-sw</a> for a list of these sensors. DriveWorks also provides the ability to integrate non-natively supported GMSL cameras. Please refer to <a class="el" href="sensorplugins_camerasipl.html">Custom Cameras (SIPL)</a> for more information.</p>
<h1><a class="anchor" id="camera_mainsection_camera_creation"></a>
Creating and Using Camera</h1>
<p >Specify the following when creating a camera using the Sensor Abstraction Layer (SAL):</p><ul>
<li>Camera protocol, which can be <code>camera.gmsl</code>, <code>camera.virtual</code>, or <code>camera.usb</code>.</li>
<li>A set of parameters specific for each camera protocol.</li>
</ul>
<p >All camera protocols have a common API, created via the SAL through a parameter string, which changes based on the protocol. Once all the camera objects have been successfully created, and the application's initialization phase is complete (<code><a class="el" href="group__sensors__common__group.html#ga4d06185ac1cf5b42f8f8762a7c564385" title="Bootstraps all sensors managed by the SAL module.">dwSAL_start()</a></code> shall be called to signal all sensors are ready to be started), start the sensors with <code><a class="el" href="group__sensors__common__group.html#ga4451ef616b16704aa57b5eda2810f8c2" title="Starts the sensor previously successfully created with &#39;dwSAL_createSensor()&#39;.">dwSensor_start()</a></code>.</p>
<dl class="section note"><dt>Note</dt><dd>Starting a sensor triggers asynchronous capture or prefetching, so it should be performed right before deciding to call <code><a class="el" href="group__camera__group.html#ga2720333fcd0dfc234863d6e4bb0a953b" title="Reads a frame handle from the camera sensor.">dwSensorCamera_readFrame()</a></code>. Camera sensors do not start right away, so check for <code>DW_NOT_READY</code> in a loop until the first instance of <code>DW_SUCCESS</code> or any failure.</dd></dl>
<p><code><a class="el" href="group__camera__group.html#ga2720333fcd0dfc234863d6e4bb0a953b" title="Reads a frame handle from the camera sensor.">dwSensorCamera_readFrame()</a></code> provides a <code><a class="el" href="group__camera__group.html#ga72de5f2a89bf7c65ed27cfb7d0805e45" title="Handle to captured frame.">dwCameraFrameHandle_t()</a></code> which is an opaque structure mapping to a Frame. A Frame is a container of images and support functions, unique to the protocol at use. The purpose of a Frame is to acquire a camera event and be able to retrieve an image suitable for the use case at hand. A call to such a function is always followed by a necessary timeout value in microseconds, due to the asynchronous nature of some of the protocols, to ensure no deadlocking happening. Frame handles are allocated into a pool of size <code>frames-pool</code> which is a parameter that can be set by the user and has value 16 by default. This means that the user can read up to frames-pool frames in flight without returning. If more than that are read, the camera will return DW_BUFFER_FULL</p>
<p >After a frame is grabbed to the application level, applications use <code><a class="el" href="group__camera__group.html#gaab9d82dffebc4a80c452937c77111fd4" title="Gets the output image/s image in a format specified by the output type.">dwSensorCamera_getImage()</a></code> to get an image in the specified output type. The available images that can be returned by this operation depend on the setup and platform. DriveWorks allows three levels of outputs:</p><ul>
<li>Native level: corresponds to the image that comes out of the lower level with as little extra operations done as possible. This is the most performant layer for memory and speed.</li>
<li>Streamed level: corresponds to images who have been streamed to CUDA in order to be unified in dwImageType, guaranteeing a common output in case of differences in setup or protocol.</li>
<li>RGBA level: a conveniently chosen format which is the result of streaming to CUDA and converting from the native format which is widely used.</li>
</ul>
<p >For a detailed table on supported output types, please refer to <a class="el" href="camera_supported_output_types.html">Supported Output Types</a>.</p>
<h2><a class="anchor" id="camera_mainsection_gmsl_cameras"></a>
GMSL Cameras (camera.gmsl)</h2>
<p >The <code>camera.gmsl</code> protocol describes GMSL cameras. These cameras acquire frames at a frequency based on the camera's frame rate For more information on image processing and management, please refer to "Understanding NvMedia" in the latest <em>NVIDIA DRIVE OS SDK Development Guide</em>.</p>
<p >This protocol is based on top of the NvSIPL library, which takes care of the driver loading and low level handling of the camera. NvSIPL connects the following components:</p><ul>
<li>Image Sensor Control (ISC).</li>
<li>Image Sensor Processing (ISP).</li>
<li>Control Algorithm that regulates exposure and white balance based on sensor statistics coming from Sensor Control and ISP.</li>
</ul>
<p >The parameters for <code>camera.gmsl</code> are: </p><pre class="fragment">camera-name = [name as specified in the SIPL database]
interface = [csi-a - csi-h | csi-ab - csi-gh | csi-a1 - csi-h1 | trio-a - trio-h | etc...]
link = [0-3]
output-format = [processed|raw|processed1|processed2]
isp-mode = [see camera.gmsl]
isp1-mode = [see camera.gmsl]
isp2-mode = [see camera.gmsl]
nito-file = [optional path to .nito file]
slave = [false/true]
isp0-res = [WidthXHeight]
isp1-res = [WidthXHeight]
isp2-res = [WidthXHeight]
CPHY-mode = [0: default DPHY Mode / 1: CPHY]
deserializer = [MAX96712: default, optional name of a deserializer to be explicitly selected]
encoder-instance=&lt;0: default or 1&gt;
enable-ultrafast-mode=&lt;0: default or 1&gt; // Only applicable to h265 encoding
skip-eeprom=&lt;0: default or 1&gt; // Skips reading EEPROM
groupInit=&lt;0: default or 1&gt; // Enable Camera Initialization in Group in only ascending link order
disable-subframe=&lt;0: default or 1&gt; // Disable ISP subframe feature which improves ISP latency, applicable to IMX623/IMX728
rec-cfg=&lt;0: default or 1: recorder support with samtec cable version-1 or 2: recorder support with samtec cable version-2&gt; // Set recording config of the deserializer
</pre><p> Each <code>dwSensorHandle_t</code> corresponds to a camera described above by name, port and link. The choice for <code>csi-port</code> depend on the sensor, especially if it is a custom one. Refer to the NvSIPL guide for more info.</p>
<p >All cameras are attached to and controlled by a master controller. By calling <code><a class="el" href="group__sensors__common__group.html#ga4451ef616b16704aa57b5eda2810f8c2" title="Starts the sensor previously successfully created with &#39;dwSAL_createSensor()&#39;.">dwSensor_start()</a></code> the first time the master controller triggers starting of all the cameras at once. During runtime, <code><a class="el" href="group__sensors__common__group.html#ga65b211709a6344856eb458c0ed128f12" title="Stops the sensor.">dwSensor_stop()</a></code> triggers the stopping of one specific sensor. If camera is live, the link is physically disabled, interrupting communication with it. If a sensor is stopped, it can then be safely released with <code>dwSensor_release()</code>. If the sensor was stopped after a malfunction, it can be reset tentatively with <code><a class="el" href="group__sensors__common__group.html#ga1ca88481b484d39642c9d3eebeebd47f" title="Resets the sensor.">dwSensor_reset()</a></code> followed by <code><a class="el" href="group__sensors__common__group.html#ga4451ef616b16704aa57b5eda2810f8c2" title="Starts the sensor previously successfully created with &#39;dwSAL_createSensor()&#39;.">dwSensor_start()</a></code> (note that after the first call, the API will only start the specific sensor). If the sequence failed, it can be retried, based on the severity of the camera failure (i.e. overheating etc...)</p>
<p >The <code>camera-name</code> parameter enables DriveWorks to tell NvSIPL which camera to use, and then load the camera drivers (under /usr/lib/nvsipl_drv). Once the drivers are loaded, the camera setup is read and communicated to DriveWorks, which then allocates the necessary resources needed by NvSIPL. Finally, a raw image coming from the sensor's ISC is fed to the ISP layer at runtime, converting it into a processed image.</p>
<p >The corresponding ISP pipeline should be enabled via output-format The <code>isp0-res</code> / <code>isp1-res</code> / <code>isp2-res</code> parameter does not have a default and the width and height to be set should not be set to a odd number.</p><ul>
<li>The <code>isp0-res</code> / <code>isp1-res</code> / <code>isp2-res</code> parameters would not be updated unless the relevant ISP pipeline is enabled via output-format.</li>
<li>Example use: isp0-res=1280X640</li>
<li>Incorrect use: isp0-res=1281X349, isp0-res=0X0</li>
</ul>
<dl class="section note"><dt>Note</dt><dd>In general, DriveWorks modules require processed images to function properly.</dd></dl>
<p>DriveWorks allocates by default image buffers that are registered with the NvSIPL library. These are in equal amounts buffers for the raw images and buffers for the processed images. As the NvSIPL library gives out images to DW, those are stored in this fifo, retreived when the user reads the frame. When the fifo is empty and the user tries to read, the camera will wait for a duration of user's timeout and then return DW_TIME_OUT. You can control the number via the <code>fifo-size</code> parameter. The user can also allocate images themselves. The properties of the alloaction can be chosen by the user or retrieved via <code><a class="el" href="group__camera__group.html#ga36d122704a33b31f450317631a9f6838" title="Gets information about the image properties for a given &#39;dwCameraImageOutputType&#39;.">dwSensorCamera_getImageProperties()</a></code>, however each camera requires a specific set of attributes in order to function correctly so the user should retrieve them also via <code>dwSensorCamera_fillImageBuffers()</code> which will populate the <code><a class="el" href="group__image__group.html#structdwImageProperties" title="Defines the properties of the image.">dwImageProperties</a></code> of the user. Note that these attributes are available only after the call to <code>dwSAL_start</code>. Once the images are created, they should be stored in a <code><a class="el" href="group__image__group.html#structdwImagePool" title="Specifies a pool of images.">dwImagePool</a></code> and passed to <code><a class="el" href="group__camera__group.html#gab39db2d665a38c14c456f7a853f7538c" title="Sets a pool of image to be used as output by the camera layer.">dwSensorCamera_setImagePool()</a></code>. This operation can be done for either or both the NATIVE camera outputs. Note that the number of images should correspond to the <code>fifo-size</code> and if it doesn't, DW will override fifo-size with the number of images in <a class="el" href="group__image__group.html#structdwImagePool" title="Specifies a pool of images.">dwImagePool</a>. Then calling <code><a class="el" href="group__sensors__common__group.html#ga4451ef616b16704aa57b5eda2810f8c2" title="Starts the sensor previously successfully created with &#39;dwSAL_createSensor()&#39;.">dwSensor_start()</a></code> will confirm if the image allocated were correct or not. Once the camera starts, NvSIPL grabs the images from the buffers and puts them to use, continuing at a rate based on the camera frame rate (in addition to some slight processing delay based on hardware specifications).</p>
<p >The asynchronous mechanism continues until the buffers run out, where all camera captures will not have an image in the buffer available for use. They are then dropped as a result, otherwise known as an ICP drop.</p>
<p >NvSIPL fills the raw image buffers and processed image buffers, and sends them back to DriveWorks. DriveWorks then stores them in a queue, in the order of their capture time.</p>
<p >You also have control over the <code><a class="el" href="group__camera__group.html#ga2720333fcd0dfc234863d6e4bb0a953b" title="Reads a frame handle from the camera sensor.">dwSensorCamera_readFrame()</a></code>, which temporarily retrieves the front of the frame queue for you. Ensure you eventually return it back to the sensor, in order to allow NvSIPL to have buffers to work with again. Generally, the buffer usage rate must be lower than the camera frame rate in order to ensure no frame is ever dropped.</p>
<p >The user can read as many frames as the <code>fifo-size</code> together and then returning them, as long as NvSIPL is not starved. A basic locking mechanism ensures that if reading is done in a multi-threaded fashion, no race condition should happen.</p>
<p >As mentioned in the table above, the image outputs available depend on the <code>output-format</code> which, by default, is <code>processed+raw</code>. Choosing only <code>processed</code> or <code>raw</code> reduces the number of resources allocated by a single camera. If you do not require raw images to be retrieved in the application level, then choosing <code>raw</code> in <code>output-format</code> is wasteful.</p>
<p >When the processed pipeline is set, NvSIPL allocates the resources needed to process the camera selected. This includes the NITO file (Nvmedia Isp Tuning Object), which contains code to correctly process a raw image. NITO files stored in the DDPX under <code>/opt/nvmedia/nit</code>, and the file corresponding to a certain camera is be computed based on the camera name.</p>
<p >Inorder to use the other processed pipelines of ISP, output-format needs to be set to processed1 and processed2. Note that the last ISP pipeline only supports rgb-fp16 output.</p>
<p >For example, <code>camera-name=SF3324</code> will search for <code>/opt/nvmedia/nit/SF3324.nito</code> and <code>/opt/nvmedia/nit/sf3324.nito</code>. If none of these are detected, DriveWorks attempts to use a generic NITO file: <code>/opt/nvmedia/nit/default.nito</code>. In most cases however, this NITO is not suitable and will likely cause an application crash. If this occurs, ensure you provide the right camera name or the path to its own NITO file. If you do not have a NITO file (e.g., working with a custom camera), you can still retrieve raw images (<code>output-format=raw</code>) but you will not be able to use DriveWorks to process them, will require a custom code to do so.</p>
<p >The <code>dwSensorSerializer</code> API allows you to record data. Depending on the <code>output-format</code> selected, the <code>camera.gmsl</code> allocates resources to record raw sensor feeds, processed video files, or both. A raw sensor feed is the unprocessed signal coming from the camera itself, and a processed video is an mp4 video of the processed output of the camera. A processed output is unchangeable and does not carry information about the sensor it was recorded with, whereas a raw signal feed can be reprocessed again, just like a live camera feed. It contains all information regarding the sensor, including its embedded metadata.</p>
<h3><a class="anchor" id="gmsl_camera_logging"></a>
Relevant logging</h3>
<p >Logging is crucial when working with NvSIPL, providing insights into whether things are working as intended. When initializing, the log should print: </p><pre class="fragment">devBlock: 1 Slave = 0 Interface = csi-a Camera_name = SF3324 Link = 0
</pre><p> This helps double check if the sensor is going to be searched correctly. A devblock corresponds to a camera port, with a total maximum of 4 blocks. If the camera is found in the database: </p><pre class="fragment">Camera Match Name: SF3324 Description: Sekonix SF3324 module - 120-deg FOV, DVP AR0231-RCCB, MAX96705 linkIndex: 4294967295 serInfo.Name: MAX96705
</pre><p> This means the driver search can be done. Ensure you should double check if this information is correct. Afterwards, when all cameras are detected, the master controller starts and prints the complete information for each camera detected if the driver was correctly loaded: </p><pre class="fragment">CameraGSMLMaster: starting...

...

Device Block : 0
    csiPort: 0
    i2cDevice: 0
    Deserializer Name: MAX96712
    Deserializer Description: Maxim 96712 Aggregator
    Deserializer i2cAddress: 41
    Simulator Mode: 0
    Slave Mode: 0
    Phy Mode: 0
    Number of camera modules: 1

...
</pre><p> These are confirmation prints, followed by prints coming directly from NvSIPL and the camera driver: </p><pre class="fragment">MAX96712: Revision 2 detected
MAX96712: Enable periodic AEQ on Link 0
MAX96705: Pre-emphasis set to 0xaa
MAX96705: Revision 1 detected!
Sensor AR0231 RCCB Rev7 detected!
</pre><p> These are completely different depending on the camera at use and the DRIVE OS version, but a good indicator that things are going well in general. Then the NITO file is searched: </p><pre class="fragment">CameraClient: no nito found at /opt/nvidia/nvmedia/nit/SF3324.nito
CameraClient: using nito found at /opt/nvidia/nvmedia/nit/sf3324.nito
</pre><p> If the NITO file is found, things can progress as explained in the sections before. Then, when <code><a class="el" href="group__sensors__common__group.html#ga4451ef616b16704aa57b5eda2810f8c2" title="Starts the sensor previously successfully created with &#39;dwSAL_createSensor()&#39;.">dwSensor_start()</a></code> is called, if things go correctly: </p><pre class="fragment">CameraMaster: bootstrap complete
</pre><p> At this point, each camera that starts acquiring frames will print: </p><pre class="fragment">CameraClient: Acquisition started
</pre><p> There are higher levels of verbosity that can be adjusted by exporting DW_SIPL_VERBOSITY=[0..4].</p>
<h2><a class="anchor" id="camera_mainsection_virtual"></a>
Virtual Cameras (camera.virtual)</h2>
<p >The <code>camera.virtual</code> protocol handles recorded data in raw or processed format. The user specifies <code>video=</code> or <code>file=</code> with the location of the data. It is possible to specify multiple <code>video=</code> or <code>file=</code> parameters, e.g. to reference a raw and a processed video. The SAL will prefer <code>video</code> over <code>file</code> parameters and use the first valid filepath, i.e. the referenced file needs to exist or it is skipped. This may be useful in cases where some video files are not always available, e.g. because one video format is provided via mounted network paths which may be slow or unreliable.</p>
<h2>Processed Videos</h2>
<p >A mp4 video is a video file that is decoded by <code>camera.virtual</code> into processed frames, identical to how they were when the sensor was being processed. This data is unchangeable and can be completely artificial data. Potentially, DriveWorks can replay any mp4 data.</p>
<p >Below is the parameter string format for when working with processed videos: </p><pre class="fragment">params: video = &lt;path.mp4&gt;
output-format = [processed only]
</pre> <h2>Raw Signal Feeds (AKA Raw videos)</h2>
<p >A raw video contains the same signals coming from the camera at the moment of capture. This means that the data is not interpretable, unless the same resources used to process the actual physical data are present at the time. In this case, the NvSIPL library uses the camera drivers to treat the frame blobs in the raw feed, as if they are sensors coming from that moment from a live camera.</p>
<p >This process is indistinguishable from a live camera; the logs, errors, sensor events, and asynchronous behavior all match the live case. For this reason, you can specify the extra parameters available in <code>camera.gmsl</code>: namely <code>isp-mode</code>, <code>fifo-size</code>, and <code>nito-file</code>. DriveWorks reads the file header to recognize the camera and its information. The name for the camera used to recover the driver is stored as a string in the header. However, you can override this by passing in the <code>camera-name</code> parameter. It is similar to the live case, where the camera requires a NITO file, and the process is the same as described above.</p>
<p >Below is the parameter string format for when working with Raw videos: </p><pre class="fragment">params: video = &lt;path.raw/lraw&gt;,output-format=[processed|raw|processed1|processed2],isp-mode=&lt;see camera.gmsl&gt;,isp1-mode=&lt;see camera.gmsl&gt;,isp2-mode=&lt;see camera.gmsl&gt;,nito-file=&lt;path.nito&gt;, fifo-size=[size],camera-name=[overridden camera name]
</pre> <h2><a class="anchor" id="camera_mainsection_gmsl_usb_ptgrey"></a>
USB Cameras (camera.usb)</h2>
<p >The <code>camera.usb</code> protocol describes USB cameras. These cameras have similar interfaces. When a sensor is started, the acquisition is performed by the OpenCV (USB) library. This library uses the OS drivers and stores the result in a buffer allocated in Camera. The driver automatically detects the frame rate for these USB cameras.</p>
<h1><a class="anchor" id="camera_mainsection_gmsl_use_cases"></a>
Relevant Tutorials</h1>
<ul>
<li><a class="el" href="camera_usecase1.html">Camera Workflow</a></li>
<li><a class="el" href="camera_usecase2.html">Integrating with Custom Board</a></li>
<li><a class="el" href="camera_usecase3.html">SIPL-based Image Sensors (Live)</a></li>
<li><a class="el" href="camera_usecase4.html">SIPL-based Image Sensors (Virtual)</a></li>
<li><a class="el" href="camera_usecase5.html">Timestamp options</a> </li>
</ul>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->

  <div id="nav-path" class="navpath">
    <ul>
      <li class="footer">
        Advance Information | Subject to Change |
        Prepared and Provided under NDA | Generated by NVIDIA |
        PR-08397-V5.0
      </li>
     </ul>
  </div>
</body>
</html>
