<!-- HTML header for doxygen 1.8.7-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<!--
 * Copyright (c) 2009-2014 NVIDIA CORPORATION.  All rights reserved.
 *
 * NVIDIA Corporation and its licensors retain all intellectual property
 * and proprietary rights in and to this software, related documentation
 * and any modifications thereto.  Any use, reproduction, disclosure or
 * distribution of this software and related documentation without an express
 * license agreement from NVIDIA Corporation is strictly prohibited.
-->
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.4"/>
<title>DriveWorks SDK Reference: DriveNet Sample</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="shortcut icon" href="Nvidia.ico" type="image/x-icon" />
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="reverb-search.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
<link href="nv.css" rel="stylesheet" type="text/css" />
<link href="nvdwx.css" rel="stylesheet" type="text/css"/>
<style>
 body {
 background-position: 350px 150px;
 background-image: url(watermark.png);
 background-repeat: no-repeat;
 background-attachment: fixed;
 }
 </style>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table width="99%" border="0" cellspacing="1" cellpadding="1">
  <tbody>
    <tr valign="middle">
      <td rowspan="2" height="44" width="19%">
        <div>
            <a id="nv-logo" href="https://www.nvidia.com/"></a>
        </div>
      <td width="81%" height="44">
        <div style="text-align:right; font-weight: bold; font-size:20px"> <br/>DriveWorks SDK Reference </div>
        <div style="text-align:right">
        5.10.87 Release <br/> For Test and Development only <br/> <br/> </div>
    </td>
  </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.4 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('dwx_drivenet_sample.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">DriveNet Sample </div></div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#dwx_drivenet_description">Description</a></li>
<li class="level1"><a href="#dwx_drivenet_sample_running">Running the Sample</a><ul><li class="level2"><a href="#dwx_drivenet_sample_examples">Examples</a></li>
</ul>
</li>
<li class="level1"><a href="#dwx_drivenet_sample_output">Output</a></li>
<li class="level1"><a href="#dwx_drivenet_sample_limitations">Limitations</a></li>
<li class="level1"><a href="#dwx_drivenet_sample_more">Additional Information</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="dwx_drivenet_description"></a>
Description</h1>
<p >The NVIDIA<sup>&reg;</sup> DriveNet sample is a sophisticated, multi-class, higher- resolution example that uses the NVIDIA<sup>&reg;</sup> DriveNet proprietary deep neural network (DNN) to perform object detection.</p>
<p >The DriveNet sample application detects objects by performing inferences on each frame of a RAW video or camera stream. It clusters these objects with parameters defined within the sample application.</p>
<p >A follow-up algorithm clusters detections from both images to compute a more stable response.</p>
<h1><a class="anchor" id="dwx_drivenet_sample_running"></a>
Running the Sample</h1>
<p >The DriveNet sample, sample_drivenet, accepts the following optional parameters. If none are specified, it performs detections on a supplied pre-recorded video. </p><pre class="fragment">./sample_drivenet --input-type=[video|camera|cameraCustom]
                  --video=[path/to/video]
                  --camera-type=[camera]
                  --camera-group=[a|b|c|d]
                  --camera-index=[0|1|2|3]
                  --cameraCustomString=[camera-parameter-string]
                  --slave=[0|1]
                  --precision=[int8|fp16|fp32]
                  --useCudaGraph=[0|1]
                  --stopFrame=[frame]
                  --enableUrgency=[0|1]
                  --stateless=[0|1]
</pre><p> Where: </p><pre class="fragment">--input-type=[video|camera|cameraCustom]
        Defines if the input is from live camera or from a recorded video.
        Live camera is supported only on NVIDIA DRIVE(tm) platforms.
        It is not supported on Linux (x86 architecture) host systems.
        Default value: video

--video=[path/to/video]
        Specifies the absolute or relative path of a raw, lraw or h264 recording.
        Only applicable if --input-type=video.
        Default value: path/to/data/samples/lraw/AR820_RGGB_8MP_v10.lraw

--camera-type=[camera]
        Only applicable if --input-type=camera.
        Default value: ar0231-rccb-bae-sf3324

--camera-group=[a|b|c|d]
        Is the group where the camera is connected to.
        Only applicable if --input-type=camera.
        Default value: a

--camera-index=[0|1|2|3]
        Indicates the camera index on the given port.
        Default value: 0

--cameraCustomString=[camera-parameter-string]
        Parameter string for custom cameras.
        Only applicable if --input-type=cameraCustom
        Default value: camera-name=SF3324,interface=csi-a,link=0,output-format=processed

--slave=[0|1]
        Setting this parameter to 1 when running the sample on Xavier B accesses the camera
        on Xavier A.
        Applicable only when --input-type=camera.
        Default value: 0

--precision=[int8|fp16|fp32]
        Defines the precision of the DriveNet DNN. The following precision levels are supported.
        - int8
          - 8-bit signed integer precision.
          - Supported GPUs: compute capability &gt;= 6.1.
          - Faster than fp16 and fp32 on GPUs with compute capability = 6.1 or compute capability &gt; 6.2.
        - fp16 (default)
          - 16-bit floating point precision.
          - Supported GPUs: compute capability &gt;= 6.2
          - Faster than fp32.
          - If fp16 is selected on a Pascal GPU, the precision will be set to fp32.
        - fp32
          - 32-bit floating point precision.
          - Supported GPUs: Only Pascal GPUs (compute capability 6.1)
          - Default for Pascal GPUs.
        When using DLA engines only fp16 is allowed.
        Default value: fp16

--useCudaGraph=[0|1]
        Setting this parameter to 1 runs Drivenet DNN inference by CUDAGraph if the hardware supports.
        Default value: 0

--stopFrame=[number]
        Runs DriveNet only on the first &lt;number&gt; frames and then exits the application.
        The default value for `--stopFrame` is 0, for which the sample runs endlessly.
        Default value: 0

--enableUrgency=[0|1]
        Enables the object urgency prediction by a temporal model.
        Only supports predicting the urgency for cars and pedestrians on the front camera with 60&amp;deg; field of view.
        Default value: 0

--stateless=[0|1]
        Setting this parameter to 0 runs the stateful temporal model. Setting it to 1 runs the stateless temporal model.
        The stateful model uses all past frames to predict urgency, while the stateless model only uses the most recent frames.
        Only applicable if --enableUrgency=1.
        Default value: 0
</pre> <h2><a class="anchor" id="dwx_drivenet_sample_examples"></a>
Examples</h2>
<h3>To run the sample on a video</h3>
<pre class="fragment">./sample_drivenet --input-type=video --video=&lt;video file.raw&gt;
</pre> <h3>To run the sample on a camera on NVIDIA DRIVE platforms</h3>
<pre class="fragment">./sample_drivenet --input-type=camera --camera-type=&lt;camera type&gt; --camera-group=&lt;camera group&gt; --camera-index=&lt;camera idx on camera group&gt;
</pre><p> where <code>&lt;camera type&gt;</code> is a supported <code>RCCB</code> sensor. See <a class="el" href="supported_sensors.html">Cameras Supported</a> for the list of supported cameras for each platform.</p>
<h3>To run the sample on a DLA engine on an NVIDIA DRIVE platform</h3>
<p >On NVIDIA DRIVE<sup>&trade;</sup> platforms, you can run DriveNet on DLA engines with the following command line: </p><pre class="fragment">./sample_drivenet --dla=1 --dlaEngineNo=0
</pre> <h3>To run the sample on a video for the first 3000 frames</h3>
<pre class="fragment">./sample_drivenet --video=&lt;video file.raw&gt; --stopFrame=3000
</pre> <h3>To run the sample with different precisions</h3>
<pre class="fragment">./sample_drivenet --precision=int8
</pre> <h3>To run the sample with urgency predictions</h3>
<pre class="fragment">./sample_drivenet --enableUrgency=1
</pre> <h1><a class="anchor" id="dwx_drivenet_sample_output"></a>
Output</h1>
<p >The sample creates a window, displays a video, and overlays bounding boxes for detected objects. The color of the bounding boxes represents the classes that the sample detects, as follows:</p>
<ul>
<li>Red: Cars and Trucks (both labeled as cars).</li>
<li>Green: Traffic Signs.</li>
<li>Blue: Bicycles.</li>
<li>Magenta: Pedestrians.</li>
<li>Orange: Traffic Lights.</li>
<li>Yellow: Curb</li>
<li>Cyan: other</li>
<li>Grey: unknown</li>
</ul>
<p >When urgency prediction is enabled, the predicted urgency value is displayed behind the object class name. The color of the bounding boxes represents urgency value with a green, white, red smoothly transitioned color map. In this color map, green indicates negative urgency, white indicates zero urgency, and red indicates positive urgency.</p>
<div class="image">
<img src="sample_drivenet.png" alt=""/>
<div class="caption">
Multiclass object detector on an RCCB stream using DriveNet</div></div>
    <h1><a class="anchor" id="dwx_drivenet_sample_limitations"></a>
Limitations</h1>
<dl class="section warning"><dt>Warning</dt><dd>DriveNet DNN currently has limitations that could affect its performance:<ul>
<li>It is optimized for daytime, clear-weather data. As a result, it does not perform well in dark or rainy conditions.</li>
<li>It is trained primarily on data collected in the United States. As a result, it may have reduced accuracy in other locales, particularly for road sign shapes that do not exist in the U.S.</li>
</ul>
</dd></dl>
<h1><a class="anchor" id="dwx_drivenet_sample_more"></a>
Additional Information</h1>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->

  <div id="nav-path" class="navpath">
    <ul>
      <li class="footer">
        Advance Information | Subject to Change |
        Prepared and Provided under NDA | Generated by NVIDIA |
        PR-08397-V5.0
      </li>
     </ul>
  </div>
</body>
</html>
