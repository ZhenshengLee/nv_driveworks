<!-- HTML header for doxygen 1.8.7-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<!--
 * Copyright (c) 2009-2014 NVIDIA CORPORATION.  All rights reserved.
 *
 * NVIDIA Corporation and its licensors retain all intellectual property
 * and proprietary rights in and to this software, related documentation
 * and any modifications thereto.  Any use, reproduction, disclosure or
 * distribution of this software and related documentation without an express
 * license agreement from NVIDIA Corporation is strictly prohibited.
-->
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.4"/>
<title>DriveWorks SDK Reference: TensorRT Optimizer Tool</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="shortcut icon" href="Nvidia.ico" type="image/x-icon" />
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="reverb-search.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  extensions: ["tex2jax.js"],
  jax: ["input/TeX","output/HTML-CSS"],
});
</script>
<script type="text/javascript" async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js"></script>
<link href="nv.css" rel="stylesheet" type="text/css" />
<link href="nvdwx.css" rel="stylesheet" type="text/css"/>
<style>
 body {
 background-position: 350px 150px;
 background-image: url(watermark.png);
 background-repeat: no-repeat;
 background-attachment: fixed;
 }
 </style>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table width="99%" border="0" cellspacing="1" cellpadding="1">
  <tbody>
    <tr valign="middle">
      <td rowspan="2" height="44" width="19%">
        <div>
            <a id="nv-logo" href="https://www.nvidia.com/"></a>
        </div>
      <td width="81%" height="44">
        <div style="text-align:right; font-weight: bold; font-size:20px"> <br/>DriveWorks SDK Reference </div>
        <div style="text-align:right">
        5.10.87 Release <br/> For Test and Development only <br/> <br/> </div>
    </td>
  </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.4 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('dwx_tensorRT_tool.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">TensorRT Optimizer Tool </div></div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#dwx_tensorRT_tool_description">Description</a></li>
<li class="level1"><a href="#dwx_tensorRT_tool_prerequisites">Prerequisites</a></li>
<li class="level1"><a href="#dwx_tensorRT_tool_usage">Running the Tool</a><ul><li class="level2"><a href="#dwx_tensorRT_tool_params">Parameters</a></li>
</ul>
</li>
<li class="level1"><a href="#dwx_tensorRT_tool_examples">Examples</a><ul><li class="level2"><a href="#dwx_tensorRT_tool_examples_uff">Optimizing UFF Models</a></li>
<li class="level2"><a href="#dwx_tensorRT_tool_examples_caffe">Optimizing Caffe Models</a></li>
<li class="level2"><a href="#dwx_tensorRT_tool_examples_onnx">Optimizing ONNX Models</a></li>
</ul>
</li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="dwx_tensorRT_tool_description"></a>
Description</h1>
<p >The NVIDIA<sup>&reg;</sup> DriveWorks TensorRT Optimizer Tool enables optimization for a given Caffe, UFF or ONNX model using TensorRT.</p>
<p >For specific examples, please refer to the following:</p>
<ul>
<li><a class="el" href="dwx_tensorRT_tool.html#dwx_tensorRT_tool_examples_uff">Optimizing UFF Models</a>.</li>
<li><a class="el" href="dwx_tensorRT_tool.html#dwx_tensorRT_tool_examples_caffe">Optimizing Caffe Models</a>.</li>
<li><a class="el" href="dwx_tensorRT_tool.html#dwx_tensorRT_tool_examples_onnx">Optimizing ONNX Models</a>.</li>
</ul>
<h1><a class="anchor" id="dwx_tensorRT_tool_prerequisites"></a>
Prerequisites</h1>
<p >This tool is available on NVIDIA DRIVE<sup>&trade;</sup> OS Linux.</p>
<p >This tool creates output files that are placed into the current working directory by default. Please ensure the following for your convenience:</p>
<ul>
<li>Write permissions are enabled for the current working directory.</li>
<li>Include the tools folder in the binary search path of the system.</li>
<li>Execute from your home directory.</li>
</ul>
<h1><a class="anchor" id="dwx_tensorRT_tool_usage"></a>
Running the Tool</h1>
<p >The TensorRT Optimization tool accepts the following parameters. Several of these parameters are required based on model type. <br  />
For more information, please refer to the <a class="el" href="dwx_tensorRT_tool.html#dwx_tensorRT_tool_examples">Examples</a>.</p>
<p >Run the tool by executing: </p><pre class="fragment">./tensorRT_optimization --modelType=[uff|caffe|onnx]
                        --outputBlobs=[&lt;output_blob1&gt;,&lt;output_blob2&gt;,...]
                        --prototxt=[path to file]
                        --caffemodel=[path to file]
                        --uffFile=[path to file]
                        --inputBlobs=[&lt;input_blob1&gt;,&lt;input_blob2&gt;,...]
                        --inputDims=[&lt;NxNxN&gt;,&lt;NxNxN&gt;,...]
                        --onxxFile=[path to file]
                        [--iterations=[int]]
                        [--batchSize=[int]]
                        [--half2=[int]]
                        [--out=[path to file]]
                        [--int8]
                        [--calib=[calibration file name]]
                        [--cudaDevice=[CUDA GPU index]]
                        [--useDLA]
                        [--useSafeDLA]
                        [--dlaLayerConfig=[path to json layer config]]
                        [--pluginConfig=[path to plugin config file]]
                        [--precisionConfig=[path to precision config file]]
                        [--testFile=[path to binary file]]
                        [--useGraph=[int]]
                        [--workspaceSize=[int]]
                        [--explicitBatch=[int]]
</pre> <h2><a class="anchor" id="dwx_tensorRT_tool_params"></a>
Parameters</h2>
<pre class="fragment">--modelType=[uff|caffe|onnx]
        Description: The type of model to be converted to the TensorRT network.
        Warning: uff and caffe model types are deprecated and will be dropped in the next major release.

--outputBlobs=[&lt;output_blob1&gt;,&lt;output_blob2&gt;,...]
        Description: Names of output blobs combined with commas.
        Example: --outputBlobs=bboxes,coverage

--prototxt=[path to file]
        Description: Deploys a file that describes the Caffe network.
        Example: --prototxt=deploy.prototxt

--caffemodel=[path to file]
        Description: Caffe model file containing weights.
        Example: --caffemodel=weights.caffemodel

--outputBlobs=[&lt;output_blob1&gt;,&lt;output_blob2&gt;,...]
        Description: Names of output blobs combined with commas.
        Example: --outputBlobs=bboxes,coverage

--uffFile=[path to file]
        Description: Path to a UFF file.
        Example: --uffFile=~/myNetwork.uff

--inputBlobs=[&lt;input_blob1&gt;,&lt;input_blob2&gt;,...]
        Description: Names of input blobs combined with commas. Ignored if the model is ONNX or Caffe.
        Example: --inputBlobs=data0,data1

--inputDims=[&lt;NxNxN&gt;,&lt;NxNxN&gt;,...]
        Description: Input dimensions for each input blob separated by commas, given in the same
                     order as the input blobs.
                     Dimensions are separated by `x`, and given in CHW format.
        Example: --inputDims=3x480x960,1x1x10

--onxxFile=[path to file]
        Description: Path to an ONNX file.
        Example: --onnxFile=~/myNetwork.onnx

--iterations=[int]
        Description: Number of iterations to run to measure speed.
                     This parameter is optional.
        Example: --iterations=100
        Default value: 10

--batchSize=[int]
        Description: Batch size of the model to be generated.
                     This parameter is optional.
        Example: --batchSize=2
        Default value: 1

--half2=[int]
        Description: The network running in paired fp16 mode. Requires platform to support native fp16.
                     This parameter is optional.
        Example: --half2=1
        Default value: 0

--out=[path to file]
        Description: Name of the optimized model file.
                     This parameter is optional.
        Example: --out=model.bin
        Default value: optimized.bin

--int8
        Description: If specified, run in INT8 mode.
                     This parameter is optional.

--calib=[calibration file name]
        Description: INT8 calibration file name.
                     This parameter is optional.
        Example: --calib=calib.cache

--cudaDevice=[CUDA GPU index]
        Description: Index of a CUDA capable GPU device.
                     This parameter is optional.
        Example: --cudaDevice=1
        Default value: 0

--verbose = [int]
        Description: Enable tensorRT verbose logging.
                     This parameter is optional
        Default value: 0

--useDLA
        Description: If specified, this generates a model to be executed on DLA. This argument is only valid on platforms with DLA hardware.
                     This parameter is optional.

--useSafeDLA
        Description: If specified, this generates a model to be executed on DLA.
                     The safe mode indicates all layers must be executable on DLA, the input/output of the DNN module
                     must be provided in the corresponding precision and format, and the input/output tensors must be provided
                     as NvMediaTensor for best performance.
                     `dwDNN` module is capable of streaming NvMediaTensors from/to CUDA and
                     converting precisions and format. For more information, please refer to `dwDNN` module's documentation.

--dlaLayerConfig
        Descripton: If specified, specific layers to be forced to GPU are read from this json. Layers to be run on GPU can be specified by type of layer or layer number. Layer type and layer number can be obtained from logs by running with default template. This argument is valid only if --useDLA=1
                    This parameter is optional.
        Example: --dlaLayerConfig=./template_dlaconfig.json

--pluginConfig=[path to plugin config file]
        Description: Path to plugin configuration file. See template_plugin.json for an example.
                     This parameter is optional.
        Example: --pluginConfig=template_plugin.json

--precisionConfig=[path to precision config file]
        Description: Path to a precision configuration file for generating models with mixed
                     precision. For layers not included in the configuration file, builder mode determines the precision. For these layers, TensorRT may choose any precision for better performance. If 'output_types' is not provided for a layer, the data type of the output tensors will be set to the precision of the layer. For the layers with precision set to INT8, scaling factors of the input/output tensors should be provided. This file can also be used to set the scaling factors for each tensor by name. The values provided in this file will override the scaling factors specified in calibration file (if provided). See 'template_precision.json' for an example.
                     This parameter is optional.
        Example: --precisionConfig=template_precision.json

--testFile=[path to binary file]
        Description: Name of a binary file for model input/output validation. This file should contain
                     flattened pairs of inputs and expected outputs in the same order as the TensorRT model expects. The file is assumed to hold 32 bit floats. The number of test pairs is automatically detected.
                     This parameter is optional.
        Example: Data with two inputs and two outputs would have a layout in the file as follows:
                 &gt; \[input 1\]\[input 2\]\[output 1\]\[output 2\]\[input 1\]\[input 2\]\[output 1\]\[output 2\]...

--useGraph
        Description: If specified, executes the optimized network by CUDA graph. It helps check if the optimized network
                     works with CUDA graph acceleration.
                     This parameter is optional.

--workspaceSize=[int]
        Description: Max workspace size in megabytes. Limits the maximum size that any layer in the network
                     can use. If insufficient scratch is provided, TensorRT may not be able to find an implementation for a given layer.
                     This parameter is optional.

--explicitBatch=[int]
        Description: Determines whether explicit batch should be enabled or not.
                     For TensorRT versions higher than or equal to 6.3, if an ONNX model is provided as
                     input, this flag will be automatically set to 1.
                     This parameter is optional.
</pre> <h1><a class="anchor" id="dwx_tensorRT_tool_examples"></a>
Examples</h1>
<h2><a class="anchor" id="dwx_tensorRT_tool_examples_uff"></a>
Optimizing UFF Models</h2>
<pre class="fragment">./tensorRT_optimization --modelType=uff
                        --outputBlobs=bboxes,coverage
                        --uffFile=~/myNetwork.uff
                        --inputBlobs=data0,data1
                        --inputDims=3x480x960,1x1x10
</pre> <h2><a class="anchor" id="dwx_tensorRT_tool_examples_caffe"></a>
Optimizing Caffe Models</h2>
<pre class="fragment">./tensorRT_optimization --modelType=caffe
                        --outputBlobs=bboxes,coverage
                        --prototxt=deploy.prototxt
                        --caffemodel=weights.caffemodel
</pre> <dl class="section note"><dt>Note</dt><dd>The <code>--inputBlobs</code> and <code>--inputDims</code> parameters are ignored if you select the Caffe model type. <br  />
All the input blobs will be automatically marked as input.</dd></dl>
<h2><a class="anchor" id="dwx_tensorRT_tool_examples_onnx"></a>
Optimizing ONNX Models</h2>
<pre class="fragment">./tensorRT_optimization --modelType=onxx
                        --onnxFile=~/myNetwork.onnx
</pre> <dl class="section note"><dt>Note</dt><dd>The <code>--inputBlobs</code>, <code>--inputDims</code>, and <code>--outBlobs</code> parameters are ignored if you select the ONNX model type.<br  />
All the input and output blobs will be automatically marked as input or output, respectively. </dd></dl>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->

  <div id="nav-path" class="navpath">
    <ul>
      <li class="footer">
        Advance Information | Subject to Change |
        Prepared and Provided under NDA | Generated by NVIDIA |
        PR-08397-V5.0
      </li>
     </ul>
  </div>
</body>
</html>
