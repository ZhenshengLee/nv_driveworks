<!-- HTML header for doxygen 1.8.13-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<!--
 * Copyright (c) 2022 NVIDIA CORPORATION.  All rights reserved.
 *
 * NVIDIA Corporation and its licensors retain all intellectual property
 * and proprietary rights in and to this software, related documentation
 * and any modifications thereto.  Any use, reproduction, disclosure or
 * distribution of this software and related documentation without an express
 * license agreement from NVIDIA Corporation is strictly prohibited.
-->
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.9.4"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Compute Graph Framework SDK Reference: Demo Pipeline Introduction</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link rel="shortcut icon" href="Nvidia.ico" type="image/x-icon" />
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="nv.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 110px;">
  <td id="projectlogo" width="19%">
    <a id="nv-logo" href="https://www.nvidia.com/"></a>
  </td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Compute Graph Framework SDK Reference
   &#160;<span id="projectnumber">5.10</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.4 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('cgf_samples_demo.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Demo Pipeline Introduction </div></div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#cgfdemo-nodes">Nodes in CGFDemo pipeline</a></li>
<li class="level1"><a href="#cgfdemo-graphlets">Graphlets in CGFDemo pipeline</a></li>
<li class="level1"><a href="#cgfdemo-dag">System DAG in CGFDemo pipeline</a></li>
<li class="level1"><a href="#cgfdemo-schedule">Schedule constraints in CGFDemo pipeline</a></li>
<li class="level1"><a href="#nvscistream-channel-feature">NvSciStream channel feature</a></li>
</ul>
</div>
<div class="textblock"><h1><a class="anchor" id="cgfdemo-nodes"></a>
Nodes in CGFDemo pipeline</h1>
<ul>
<li><b>RenderingCGFDemoNode</b>: Rendering node for the demo pipeline. The node is described by the RenderingCGFDemoNode.node.json file</li>
<li><b>dwRadarNode</b>: 5 instantiations of this node to create 5 radar inputs. Each node is described by the dwRadarNode.node.json file</li>
<li><b>dwIMUNode</b>: 1 instantiation of this node to create 1 IMU input. This node is described by the dwIMUNode.node.json file</li>
<li><b>dwVehicleStateNode</b>: 1 instantiation of this node, described by the dwVehicleStateNode.node.json file</li>
<li><b>dwCameraNode</b>: 4 instantiations of this node to create four camera inputs. The node is described by the dwCameraNode.node.json file</li>
<li><b>SensorSyncNode</b>: this node is responsible for sensor inputs synchronization. This node is described by the SensorSyncNode.node.json file</li>
<li><b>dwRelativeEgomotionIMUNode</b>: this node is responsible to generate egomotion, described by the dwRelativeEgomotionIMUNode.node.json file</li>
<li><b>dwRadarDopplerMotionNode</b>: this node post processes radar sensor outputs for self-calibration, described by dwRadarDopplerMotionNode.node.json</li>
<li><b>dwPyramidNode</b>: this node prepares the images in pyramid representation for feature detection purposes, described in dwPyramidNode.node.json file</li>
<li><b>dwFeatureDetectionNode</b>: this node is responsible for feature detection, described by the dwFeatureDetectorNode.node.json file</li>
<li><b>dwFeatureTrackerNode</b>: this node is responsible for feature tracking, described by the dwFeatureTrackerNode.node.json file</li>
<li><b>dwSelfCalibrationNode</b>: this node is responsible to generate self-calibration results, described by the dwSelfCalibrationNode.node.json</li>
</ul>
<h1><a class="anchor" id="cgfdemo-graphlets"></a>
Graphlets in CGFDemo pipeline</h1>
<ul>
<li><b>RenderDemo graphlet</b>: this graphlet contains RenderingCGFDemoNode node</li>
<li><b>RadarSensor graphlet</b>: this graphlet contains dwRadarnode node</li>
<li><b>ImuSensor graphlet</b>: this graphlet contains dwIMUnode node</li>
<li><b>VehicleStateConsumer graphlet</b>: this graphlet contains dwVehicleStatenode node</li>
<li><b>CameraSensor graphlet</b>: this graphlet contains dwCameraNode and ISPNode nodes. Camera frames are fed through dwCameraNode and ISPNode to provide demosaiced image outputs to post processing in later pipeline</li>
<li><b>SensorSync graphlet</b>: this graphlet contains SensorSyncNode node</li>
<li><b>EgomotionDemo graphlet</b>: this graphlet contains two dwRelativeEgomotionIMUNode nodes. One of the egomotion is responsible for outputting egomotion state as odometry whereas the other egomotion node generates egomotion state that later feeds into self calibration graphlet</li>
<li><b>RadarDopplerMotion graphlet</b>: this graphlet contains eight instantiation of dwRadarDopplerMotionNodes</li>
<li><b>CameraPreprocessingDemo graphlet</b>: this graphlet contains dwPyramidNode, dwFeatureDetectionNode, and dwFeatureTrackerNode. The data feeds through these three nodes to produce feature tracking that are fed into self calibration and rendering nodes</li>
<li><b>CameraPipeline graphlet</b>: this graphlet contains three sub-graphlets, CameraSensor, CameraPreprocessingDemo, and CameraObjectDetectorDemo graphlets</li>
<li><b>SelfCalibrationDemo graphlet</b>: this graphlet contains dwSelfCalibrationNode node</li>
<li><b>CGFDemo graphlet</b>: this graphlet is the main graphlet that connects all graphlets in this demo</li>
</ul>
<h1><a class="anchor" id="cgfdemo-dag"></a>
System DAG in CGFDemo pipeline</h1>
<p >In this section, we will look at the demo structure in more detail. The demo uses four cameras and a DNN based object detection node. Below is a block diagram of this demo:</p>
<div class="image">
<img src="cgf-demo-diagram.png" alt=""/>
<div class="caption">
CGF Demo Diagram</div></div>
    <p >A graphical editor tool, known as the DW Graph UI tool, can also be used to open the graphlet JSON file for visualization. Please use the released graphical editor tool released to open CGFDemo.graphlet.json file. An example of the graphical output of this demo is shown below:</p>
<div class="image">
<img src="cgf-demo-dag.png" alt=""/>
<div class="caption">
CGF Demo DAG</div></div>
    <p >Zooming into camera graphlet in the compute graph:</p>
<div class="image">
<img src="cgf-demo-dag-zoom.png" alt=""/>
<div class="caption">
CGF Demo Camera Graphlet</div></div>
    <p >As described earlier, the cameraSensor graphlet contains two nodes, dwCameraNode and ISPNode. By double clicking on the cameraSensor box, a new tab in the GUI tool will be created showing the nodes inside the camera graphlet shown as below:</p>
<div class="image">
<img src="cgf-demo-camera-graphlet.png" alt=""/>
<div class="caption">
CGF Demo Camera Graphlet</div></div>
    <p >The division of the processes is defined in CGFDemo.schedule.json file. The processes are divided as the following:</p>
<ul>
<li>Camera_master process<ul>
<li>Subcomponents - SelfCalibration, radarSensor0, radarSensor1, radarSensor2, radarSensor3, radarSensor4, radarDopplerMotion, imuSensor, vehicleStateConsumer, vehicleStateConsumer, egomotion, cameraEpochSync, radarEpochSync, imuCanEpochSync</li>
</ul>
</li>
<li>Render process<ul>
<li>Subcomponents - arender</li>
</ul>
</li>
<li>Camera_pipeline0 process<ul>
<li>Subcomponents - cameraPipeline0</li>
</ul>
</li>
<li>Camera_pipeline1 process<ul>
<li>Subcomponents - cameraPipeline1</li>
</ul>
</li>
<li>Camera_pipeline2 process<ul>
<li>Subcomponents - cameraPipeline2</li>
</ul>
</li>
<li>Camera_pipeline3 process<ul>
<li>Subcomponents - cameraPipeline3</li>
</ul>
</li>
</ul>
<p >There are four instances of camera pipelines: cameraPipeline0 - 3. Each pipeline is defined in the CameraPipeline graphlet JSON file. In the CameraPipeline graphlet JSON, it includes three sub-graphlets: CameraSensor, CameraPreProcessingDemo, and CameraObjectDetectorDemo. To view the graphlet in graphical form, you can use the GUI editor tool or simply check the connections section of JSON to determine how the graphlets are connected.</p>
<p >The DNN implementation is based on the sample application "sample_object_detector_tracker". It uses tensorRT_model.bin from the sample app to perform object detection on four 2MP camera recordings. The C++ implementation of this node follows the code structure defined previously in the custom node section.</p>
<p >To render the output of the detection node onto display, an input, CUSTOM_OBJECT_DETECTION_ARRAY, exists in RenderingCGFDemoNode to receive the output of CameraObjectDetectorCGFDemoNode node. The connection takes in an array of detected bounding boxes.</p>
<p >To launch the demo, copy nvsciipc.cfg from <code>targets/&lt;x86_64-Linux or aarch64-Linux&gt;/config/nvsciipc.cfg</code> into /etc/ folder and run script in <code>bin/config/run_cgf.sh</code>. Use the following command to launch the demo:</p>
<p >"sudo ./run_cgf.sh -p /usr/local/driveworks/src/cgf/graphs/descriptions/systems/CGFDemo.system.json -s /usr/local/driveworks/src/cgf/graphs/CGFDemo.stm"I</p>
<p >The default view will show the following:</p>
<div class="image">
<img src="cgf-demo-cam1.png" alt=""/>
<div class="caption">
CGF Demo Single Camera View</div></div>
    <p >To view feature tracker rendering, use the following key sequence "nvidia", then 'h' key. To view self-calibration status, press F12 button</p>
<h1><a class="anchor" id="cgfdemo-schedule"></a>
Schedule constraints in CGFDemo pipeline</h1>
<p >For each pass defined in CGFDemo pipeline to be modeled as an STM runnable, the workload must satisfy the following requirements:</p>
<ol type="1">
<li>Each runnable must run on an independent hardware engine. Synchronous workloads are not permitted. They must be reconstituted as CPU runnables that submit another runnable, the async engine workload, to the engine's queue. The runnable that submits the workload is not allowed to wait for the completion of the submitted workload. Instead, another runnable can depend on the submitted workload and can wait for the submitted workload’s completion.</li>
<li>For each runnable, the following must be specified (please refer to term definitions next page):<ol type="a">
<li>Periodicity (Epoch, Hyperepoch)</li>
<li>Worst Case Execution Time (WCET)</li>
<li>Hardware Engine</li>
<li>Software Resources (if any)</li>
<li>Process annotation</li>
</ol>
</li>
<li>Data errors will not be handled by STM. They must be handled by the application's runnables.</li>
</ol>
<p >In the following, we will take nvidia_computegraphframework_linux-amd64-ubuntu/apps/roadrunner-2.0/graphs/descriptions/systems/CGFDemo.schedule.json as an example for further elaboration.</p>
<ol type="1">
<li><b>Global resources</b><ol type="a">
<li>Resources that are used system-wide are modeled under the global resources section.</li>
<li>The CPU, GPU, DLA and VPI resource types are known resource types for the compiler, and it will take specialized scheduling steps for runnables scheduled on those resources. Other resource types are considered as scheduling mutexes, and they do not have any naming restrictions. <pre class="fragment">{
    "resources": {
        "TegraA": {
            "CPU": ["CPU0", "CPU1", "CPU2", "CPU3", "CPU4", "CPU5"],
            "GPU": ["dGPU"]
        },
        "TegraB": {
        }
    },
    ...
}
</pre></li>
</ol>
</li>
<li><b>WCET</b><ol type="a">
<li>"wcet" : "./CGFDemo_wcet.yaml".</li>
<li>The framework assumes that passes have a bounded runtime. Worst cast of execution time of each pass is defined in CGFDemo_wcet.yaml.</li>
</ol>
</li>
<li><b>Hyperepochs</b><ol type="a">
<li>Hyperepoch is a resource partition that runs a fixed configuration of epochs that share the resources in that partition. It is periodic in nature, and it respawns the contained epochs at the specified period. <pre class="fragment">"hyperepochs": {
    "cameraHyperepoch": {
        "resources": [
            "TegraA.CPU0",
            "TegraA.CPU1",
            "TegraA.CPU2",
            "TegraA.CPU3",
            "TegraA.dGPU",
            "camera_master.TegraA.CUDA_STREAM0",
            "camera_master.TegraA.CUDA_MUTEX_LOCK",
            "camera_pipeline0.TegraA.CUDA_STREAM0",
            "camera_pipeline0.TegraA.CUDA_MUTEX_LOCK",
            "camera_pipeline1.TegraA.CUDA_STREAM0",
            "camera_pipeline1.TegraA.CUDA_MUTEX_LOCK",
            "camera_pipeline2.TegraA.CUDA_STREAM0",
            "camera_pipeline2.TegraA.CUDA_MUTEX_LOCK",
            "camera_pipeline3.TegraA.CUDA_STREAM0",
            "camera_pipeline3.TegraA.CUDA_MUTEX_LOCK"
        ],
        "period": 33000000,
        "epochs": {
            "renderEpoch": {
                "period": 33000000,
                "frames": 1,
                "passes": [
                    "arender"
                ]
            },
            "cameraEpoch": {
                "period": 33000000,
                "frames": 1,
                "passes": [
                    "cameraEpochSync",
                    "cameraPipeline0",
                    "cameraPipeline1",
                    "cameraPipeline2",
                    "cameraPipeline3",
                    "selfCalibration"
                ]
            },
            "radarDopplerMotionEpoch": {
                "period": 33000000,
                "frames": 1,
                "passes": [
                    "radarDopplerMotion"
                ]
            }
        }
    },
    "imuHyperepoch": {
        ...
    }
}
</pre></li>
<li>Each hyperepoch is associated with a mutually exclusive set of resources. <b>Resources</b> are mapped to hyperepochs by specifying the resource IDs in a list under the ‘Resources’ heading inside the hyperepoch specification.</li>
<li>The <b>period</b> for a hyperepoch specifies the rate at which the contained epochs are spawned.</li>
<li><b>Epochs</b> are timebases at which rate constituent passes spawn confined to the boundaries of the hyperepoch. Each epoch is a member of a hyperepoch, and has two attributes associated with it - Period and Frames.</li>
</ol>
</li>
<li><b>Processes</b><ol type="a">
<li>Hyperepochs and epochs define the timing boundaries for tasks (passes). Clients define the data boundaries. A process is an operating system process that contains software resources (like CUDA streams) and passes. <pre class="fragment">"processes": {
    "camera_pipeline0": {
        "executable": "LoaderLite",
        "subcomponents": [
            "cgfDemo.cameraPipeline0",
            "cgfDemo.arender"
        ]
    },
    "camera_pipeline1": {
        "executable": "LoaderLite",
        "subcomponents": [
            "cgfDemo.cameraPipeline1"
        ]
    },
    "camera_pipeline2": {
        "executable": "LoaderLite",
        "subcomponents": [
            "cgfDemo.cameraPipeline2"
        ]
    },
    "camera_pipeline3": {
        "executable": "LoaderLite",
        "subcomponents": [
            "cgfDemo.cameraPipeline3"
        ]
    },
    "camera_master": {
        "executable": "LoaderLite",
        "subcomponents": [
            "cgfDemo.selfCalibration",
            "cgfDemo.radarSensor0",
            "cgfDemo.radarSensor1",
            "cgfDemo.radarSensor2",
            ...
        ]
    }
    ...
}
</pre></li>
<li>Clients can specify resources that are visible to passes locally. These resources cannot be accessed by passes in other clients. Global resources are visible to all passes. Process-specific resources like CUDA streams and DLA/VPI handles are some examples of client resources that cannot be shared across different clients. Also, internal scheduling mutexes can also be modeled here.</li>
<li>In the demo pipeline, there are 6 STM clients (6 operating system processes):<ul>
<li>render</li>
<li>camera_pipeline0</li>
<li>camera_pipeline1</li>
<li>camera_pipeline2</li>
<li>camera_pipeline3</li>
<li>camera_master</li>
</ul>
</li>
</ol>
</li>
</ol>
<h1><a class="anchor" id="nvscistream-channel-feature"></a>
NvSciStream channel feature</h1>
<p >NvSciStream channel feature in the release, that supports a single producer and a single consumer, is enabled. Some of the channels with a single producer and a single consumer have been updated to NvSciStream in the demo. In graphlet file CGFDemo.graphlet.json, in the connection section, type parameter were changed from socket to nvsci: </p><pre class="fragment">"connections": [
    {
        "src": "cameraPipeline0.NEXT_IMAGE_TIMESTAMP",
        "dests": {"cameraEpochSync.SENSOR_TIMESTAMP[0]": {}},
        "params": {"type": "nvsci", "indirect": true}
    },
    ...
]
</pre><p> In addition, nvsciipc.cfg file under directory /etc was modified to add additional ipc slots for NvSciStream to use. Each additional line is a slot allocated for each producer/consumer NvSciStream channel: </p><pre class="fragment">INTER_PROCESS cgf_0_p cgf_0_c 16 24576
</pre><p> The naming convention is cgf*&lt;connection number&gt;*&lt;producer/consumer&gt;</p>
<p >Please make sure there are 32 slots allocated inside the nvsciipc.cfg file. After the file has been updated, it is necessary to reboot the system for changes to take effect. To make sure the changes has been properly applied, please check with the following command:</p>
<p ><b>sudo service nv_nvsciipc_init status</b> should print message below:</p>
<blockquote class="doxtable">
<p >&zwj;nv_nvsciipc_init.service - NvSciIpc initialization</p>
<p >Loaded: loaded (/lib/systemd/system/nv_nvsciipc_init.service; enabled; vendor preset: enabled)</p>
<p >Active: inactive (dead) since Wed 2021-06-16 20:36:43 UTC; 52min ago</p>
<p >Process: 758 ExecStart=/bin/sh /etc/systemd/scripts/nv_nvsciipc_init.sh (code=exited, status=0/SUCCESS)</p>
<p >Main PID: 758 (code=exited, status=0/SUCCESS) </p>
</blockquote>
<p>If not, please double check your nvsciipc.cfg file. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.4 </li>
  </ul>
</div>
</body>
</html>
